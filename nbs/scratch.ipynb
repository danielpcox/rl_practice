{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f93d09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# HACK because multinomial not yet implemented on MPS\n",
    "import os\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK']=\"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import scipy.signal\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b076680e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0.dev20220629\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "useMPS = False\n",
    "if useMPS and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # mps for my M1 Mac\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02839873",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hyperparameters and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d7d540c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "gamma = 0.99\n",
    "lr = 1e-4\n",
    "hid_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bfbdfce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_dim (210, 160, 3)\n",
      "action_dim 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n",
      "/Users/danielpcox/.pyenv/versions/rl_practice/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001B[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env_name = \"ALE/Pong-v5\"\n",
    "env = gym.make(env_name,\n",
    "               render_mode='rgb_array'\n",
    "#                render_mode='human'\n",
    "              )\n",
    "\n",
    "obs_dim = env.observation_space.shape\n",
    "print(\"obs_dim\", obs_dim)\n",
    "\n",
    "action_dim = env.action_space.n\n",
    "print(\"action_dim\", action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f9932",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a2fcfe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCritic(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(6, 6), stride=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Tanh()\n",
      "    (3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (4): Tanh()\n",
      "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (6): Tanh()\n",
      "  )\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=2304, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "  )\n",
      "  (actor_head): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (critic_head): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh\n",
    "        self.last_obs = np.zeros((80,80))\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=6, stride=2),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            activation(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            activation(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            activation(),\n",
    "        )\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2304, hid_dim),\n",
    "            activation(),\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            activation(),\n",
    "        )\n",
    "        \n",
    "        self.actor_head = nn.Linear(hid_dim, action_dim)\n",
    "        \n",
    "        self.critic_head = nn.Linear(hid_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO add conv layers\n",
    "#         set_trace()\n",
    "        body = self.preprocess(x)\n",
    "        body = self.cnn(body)\n",
    "        body = body.reshape(-1)\n",
    "        body = self.mlp(body)\n",
    "        \n",
    "        logits = self.actor_head(body)\n",
    "        pi = torch.distributions.Categorical(logits=logits)\n",
    "        action = pi.sample()\n",
    "        logp = pi.log_prob(action)\n",
    "        \n",
    "        state_value = self.critic_head(body).squeeze()\n",
    "        \n",
    "        return action.item(), logp, state_value\n",
    "    \n",
    "    def preprocess(self, obs):\n",
    "        \"\"\"Preprocess Pong observation\"\"\"\n",
    "\n",
    "        # slice off top and bottom, and downsample\n",
    "        obs = obs[34:194:2,::2,2]\n",
    "\n",
    "        # result is diff between two frames to make easier for NN w/o memory\n",
    "        result = obs - self.last_obs\n",
    "\n",
    "        # Simplify values to {0, 1}\n",
    "        result[result != 0] = 1\n",
    "        result[result != 1] = 0\n",
    "\n",
    "        self.last_obs = obs\n",
    "\n",
    "        result = torch.as_tensor(result,\n",
    "                                 dtype=torch.float32,\n",
    "                                 device=device)\n",
    "        result = result.unsqueeze(dim=0)\n",
    "        \n",
    "        return result\n",
    "\n",
    "agent = ActorCritic().to(device)\n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24bbcc36",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(agent.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0dcdb6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_returns(rewards, discount):\n",
    "    # https://stackoverflow.com/a/47971187/379547\n",
    "    r = rewards[::-1]\n",
    "    a = [1, -discount]\n",
    "    b = [1]\n",
    "    y = scipy.signal.lfilter(b, a, x=r)\n",
    "    y = y[::-1].copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return torch.as_tensor(y, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71346b40",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss(returns, logprobs, values):\n",
    "    actor_loss = -(logprobs * returns).mean()\n",
    "    critic_loss = -(values - returns).pow(2).mean()\n",
    "    # TODO add entropy bonus\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303b2592",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    logprobs = []\n",
    "    values = []\n",
    "    while not done:\n",
    "        action, logp, value = agent(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        logprobs.append(logp)\n",
    "        values.append(value)\n",
    "    \n",
    "    returns = get_returns(rewards, discount=gamma)\n",
    "    logprobs = torch.stack(logprobs)\n",
    "    values = torch.stack(values)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = get_loss(returns, logprobs, values)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return torch.as_tensor(rewards).sum().item(), loss.item(), len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d032e30",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1 µs, total: 1 µs\n",
      "Wall time: 3.1 µs\n",
      "0 Rewards:-20.0, Loss:-8.32459, Ep.Length:927\n",
      "1 Rewards:-21.0, Loss:-8.24611, Ep.Length:960\n",
      "2 Rewards:-21.0, Loss:-10.17312, Ep.Length:843\n",
      "3 Rewards:-21.0, Loss:-11.78729, Ep.Length:765\n",
      "4 Rewards:-21.0, Loss:-11.82553, Ep.Length:783\n",
      "5 Rewards:-20.0, Loss:-6.62542, Ep.Length:1210\n",
      "6 Rewards:-21.0, Loss:-10.64589, Ep.Length:854\n",
      "7 Rewards:-21.0, Loss:-10.85267, Ep.Length:853\n",
      "8 Rewards:-20.0, Loss:-8.84638, Ep.Length:1008\n",
      "9 Rewards:-17.0, Loss:-8.33974, Ep.Length:1113\n",
      "10 Rewards:-20.0, Loss:-9.75285, Ep.Length:1010\n",
      "11 Rewards:-20.0, Loss:-10.48933, Ep.Length:928\n",
      "12 Rewards:-21.0, Loss:-13.59065, Ep.Length:825\n",
      "13 Rewards:-20.0, Loss:-14.30078, Ep.Length:861\n",
      "14 Rewards:-21.0, Loss:-15.26849, Ep.Length:855\n",
      "15 Rewards:-18.0, Loss:-9.81456, Ep.Length:1146\n",
      "16 Rewards:-21.0, Loss:-15.08982, Ep.Length:902\n",
      "17 Rewards:-21.0, Loss:-13.33950, Ep.Length:1051\n",
      "18 Rewards:-20.0, Loss:-14.20237, Ep.Length:1007\n",
      "19 Rewards:-20.0, Loss:-17.51690, Ep.Length:935\n",
      "20 Rewards:-21.0, Loss:-21.65091, Ep.Length:793\n",
      "21 Rewards:-20.0, Loss:-19.29426, Ep.Length:872\n",
      "22 Rewards:-21.0, Loss:-22.62291, Ep.Length:803\n",
      "23 Rewards:-19.0, Loss:-15.44673, Ep.Length:1190\n",
      "24 Rewards:-21.0, Loss:-24.32130, Ep.Length:794\n",
      "25 Rewards:-21.0, Loss:-24.30790, Ep.Length:812\n",
      "26 Rewards:-21.0, Loss:-21.12045, Ep.Length:1001\n",
      "27 Rewards:-21.0, Loss:-22.91072, Ep.Length:929\n",
      "28 Rewards:-20.0, Loss:-22.34093, Ep.Length:905\n",
      "29 Rewards:-19.0, Loss:-20.93246, Ep.Length:1011\n",
      "30 Rewards:-19.0, Loss:-21.78861, Ep.Length:950\n",
      "31 Rewards:-21.0, Loss:-25.42590, Ep.Length:857\n",
      "32 Rewards:-21.0, Loss:-27.77832, Ep.Length:784\n",
      "33 Rewards:-18.0, Loss:-18.13485, Ep.Length:1120\n",
      "34 Rewards:-21.0, Loss:-24.36691, Ep.Length:933\n",
      "35 Rewards:-21.0, Loss:-26.72204, Ep.Length:885\n",
      "36 Rewards:-20.0, Loss:-25.10086, Ep.Length:868\n",
      "37 Rewards:-20.0, Loss:-23.82701, Ep.Length:931\n",
      "38 Rewards:-21.0, Loss:-23.72202, Ep.Length:1024\n",
      "39 Rewards:-19.0, Loss:-21.97119, Ep.Length:1038\n",
      "40 Rewards:-21.0, Loss:-29.47582, Ep.Length:793\n",
      "41 Rewards:-21.0, Loss:-30.69934, Ep.Length:765\n",
      "42 Rewards:-21.0, Loss:-30.92500, Ep.Length:765\n",
      "43 Rewards:-21.0, Loss:-31.14772, Ep.Length:765\n",
      "44 Rewards:-19.0, Loss:-21.65859, Ep.Length:1088\n",
      "45 Rewards:-19.0, Loss:-22.60784, Ep.Length:1042\n",
      "46 Rewards:-21.0, Loss:-29.65826, Ep.Length:825\n",
      "47 Rewards:-21.0, Loss:-27.17974, Ep.Length:931\n",
      "48 Rewards:-21.0, Loss:-27.67530, Ep.Length:913\n",
      "49 Rewards:-21.0, Loss:-28.76039, Ep.Length:881\n",
      "50 Rewards:-21.0, Loss:-26.61746, Ep.Length:972\n",
      "51 Rewards:-21.0, Loss:-31.58871, Ep.Length:826\n",
      "52 Rewards:-21.0, Loss:-30.40805, Ep.Length:845\n",
      "53 Rewards:-20.0, Loss:-27.87293, Ep.Length:931\n",
      "54 Rewards:-20.0, Loss:-28.91379, Ep.Length:871\n",
      "55 Rewards:-21.0, Loss:-29.08564, Ep.Length:934\n",
      "56 Rewards:-17.0, Loss:-22.10335, Ep.Length:1090\n",
      "57 Rewards:-20.0, Loss:-31.63976, Ep.Length:872\n",
      "58 Rewards:-21.0, Loss:-31.48677, Ep.Length:853\n",
      "59 Rewards:-19.0, Loss:-24.56620, Ep.Length:1133\n",
      "60 Rewards:-21.0, Loss:-34.97448, Ep.Length:765\n",
      "61 Rewards:-21.0, Loss:-32.33405, Ep.Length:844\n",
      "62 Rewards:-21.0, Loss:-30.12627, Ep.Length:928\n",
      "63 Rewards:-19.0, Loss:-24.85070, Ep.Length:1130\n",
      "64 Rewards:-21.0, Loss:-33.81083, Ep.Length:825\n",
      "65 Rewards:-21.0, Loss:-32.90619, Ep.Length:853\n",
      "66 Rewards:-21.0, Loss:-36.27262, Ep.Length:765\n",
      "67 Rewards:-21.0, Loss:-31.69953, Ep.Length:913\n",
      "68 Rewards:-21.0, Loss:-34.82396, Ep.Length:825\n",
      "69 Rewards:-21.0, Loss:-29.50404, Ep.Length:1007\n",
      "70 Rewards:-20.0, Loss:-31.02892, Ep.Length:905\n",
      "71 Rewards:-20.0, Loss:-31.18395, Ep.Length:931\n",
      "72 Rewards:-21.0, Loss:-35.89040, Ep.Length:825\n",
      "73 Rewards:-21.0, Loss:-37.83339, Ep.Length:765\n",
      "74 Rewards:-20.0, Loss:-30.71821, Ep.Length:960\n",
      "75 Rewards:-20.0, Loss:-30.82974, Ep.Length:987\n",
      "76 Rewards:-21.0, Loss:-38.49715, Ep.Length:765\n",
      "77 Rewards:-21.0, Loss:-38.69688, Ep.Length:765\n",
      "78 Rewards:-21.0, Loss:-32.76826, Ep.Length:945\n",
      "79 Rewards:-21.0, Loss:-35.82811, Ep.Length:853\n",
      "80 Rewards:-21.0, Loss:-39.35079, Ep.Length:765\n",
      "81 Rewards:-20.0, Loss:-33.13661, Ep.Length:914\n",
      "82 Rewards:-21.0, Loss:-39.78453, Ep.Length:765\n",
      "83 Rewards:-21.0, Loss:-39.99911, Ep.Length:765\n",
      "84 Rewards:-21.0, Loss:-40.23788, Ep.Length:765\n",
      "85 Rewards:-19.0, Loss:-30.61258, Ep.Length:1002\n",
      "86 Rewards:-21.0, Loss:-38.45673, Ep.Length:825\n",
      "87 Rewards:-21.0, Loss:-36.54768, Ep.Length:888\n",
      "88 Rewards:-20.0, Loss:-31.83160, Ep.Length:1017\n",
      "89 Rewards:-21.0, Loss:-40.17943, Ep.Length:793\n",
      "90 Rewards:-19.0, Loss:-31.33151, Ep.Length:1014\n",
      "91 Rewards:-20.0, Loss:-32.32731, Ep.Length:1025\n",
      "92 Rewards:-21.0, Loss:-41.94413, Ep.Length:765\n",
      "93 Rewards:-21.0, Loss:-42.16564, Ep.Length:765\n",
      "94 Rewards:-21.0, Loss:-42.37562, Ep.Length:765\n",
      "95 Rewards:-21.0, Loss:-42.57604, Ep.Length:765\n",
      "96 Rewards:-21.0, Loss:-38.96470, Ep.Length:882\n",
      "97 Rewards:-21.0, Loss:-40.45366, Ep.Length:827\n",
      "98 Rewards:-21.0, Loss:-40.84901, Ep.Length:825\n",
      "99 Rewards:-21.0, Loss:-43.42516, Ep.Length:765\n",
      "100 Rewards:-21.0, Loss:-42.38568, Ep.Length:793\n",
      "101 Rewards:-20.0, Loss:-38.55784, Ep.Length:862\n",
      "102 Rewards:-21.0, Loss:-44.07849, Ep.Length:765\n",
      "103 Rewards:-21.0, Loss:-38.66447, Ep.Length:915\n",
      "104 Rewards:-21.0, Loss:-38.30052, Ep.Length:933\n",
      "105 Rewards:-21.0, Loss:-44.72659, Ep.Length:765\n",
      "106 Rewards:-19.0, Loss:-33.69737, Ep.Length:1044\n",
      "107 Rewards:-21.0, Loss:-45.15659, Ep.Length:765\n",
      "108 Rewards:-21.0, Loss:-43.25185, Ep.Length:813\n",
      "109 Rewards:-21.0, Loss:-41.20588, Ep.Length:873\n",
      "110 Rewards:-21.0, Loss:-44.58810, Ep.Length:793\n",
      "111 Rewards:-19.0, Loss:-37.07819, Ep.Length:938\n",
      "112 Rewards:-20.0, Loss:-38.11798, Ep.Length:952\n",
      "113 Rewards:-19.0, Loss:-34.91546, Ep.Length:1060\n",
      "114 Rewards:-19.0, Loss:-37.86846, Ep.Length:934\n",
      "115 Rewards:-21.0, Loss:-41.94807, Ep.Length:885\n",
      "116 Rewards:-21.0, Loss:-44.66346, Ep.Length:821\n",
      "117 Rewards:-21.0, Loss:-47.23257, Ep.Length:765\n",
      "118 Rewards:-20.0, Loss:-38.34381, Ep.Length:992\n",
      "119 Rewards:-20.0, Loss:-41.93411, Ep.Length:871\n",
      "120 Rewards:-21.0, Loss:-44.06843, Ep.Length:853\n",
      "121 Rewards:-21.0, Loss:-48.03127, Ep.Length:765\n",
      "122 Rewards:-21.0, Loss:-48.23767, Ep.Length:765\n",
      "123 Rewards:-21.0, Loss:-47.02938, Ep.Length:814\n",
      "124 Rewards:-21.0, Loss:-48.65232, Ep.Length:765\n",
      "125 Rewards:-21.0, Loss:-48.82312, Ep.Length:765\n",
      "126 Rewards:-20.0, Loss:-39.01241, Ep.Length:1027\n",
      "127 Rewards:-21.0, Loss:-48.05054, Ep.Length:793\n",
      "128 Rewards:-20.0, Loss:-42.60156, Ep.Length:896\n",
      "129 Rewards:-21.0, Loss:-49.68306, Ep.Length:765\n",
      "130 Rewards:-21.0, Loss:-44.03102, Ep.Length:932\n",
      "131 Rewards:-21.0, Loss:-47.31098, Ep.Length:826\n",
      "132 Rewards:-21.0, Loss:-50.27269, Ep.Length:765\n",
      "133 Rewards:-20.0, Loss:-43.47926, Ep.Length:898\n",
      "134 Rewards:-21.0, Loss:-50.69477, Ep.Length:765\n",
      "135 Rewards:-21.0, Loss:-50.90391, Ep.Length:765\n",
      "136 Rewards:-20.0, Loss:-46.05445, Ep.Length:844\n",
      "137 Rewards:-21.0, Loss:-51.32610, Ep.Length:765\n",
      "138 Rewards:-21.0, Loss:-51.50227, Ep.Length:765\n",
      "139 Rewards:-21.0, Loss:-44.73521, Ep.Length:943\n",
      "140 Rewards:-21.0, Loss:-47.88150, Ep.Length:855\n",
      "141 Rewards:-21.0, Loss:-52.09744, Ep.Length:765\n",
      "142 Rewards:-21.0, Loss:-48.34171, Ep.Length:865\n",
      "143 Rewards:-21.0, Loss:-52.52559, Ep.Length:765\n",
      "144 Rewards:-21.0, Loss:-52.71282, Ep.Length:765\n",
      "145 Rewards:-21.0, Loss:-52.93652, Ep.Length:765\n",
      "146 Rewards:-21.0, Loss:-53.10286, Ep.Length:765\n",
      "147 Rewards:-21.0, Loss:-53.31185, Ep.Length:765\n",
      "148 Rewards:-21.0, Loss:-47.91100, Ep.Length:896\n",
      "149 Rewards:-21.0, Loss:-52.81341, Ep.Length:784\n",
      "150 Rewards:-21.0, Loss:-53.90121, Ep.Length:765\n",
      "151 Rewards:-21.0, Loss:-54.11079, Ep.Length:765\n",
      "152 Rewards:-21.0, Loss:-49.54130, Ep.Length:883\n",
      "153 Rewards:-21.0, Loss:-54.53151, Ep.Length:765\n",
      "154 Rewards:-21.0, Loss:-54.73733, Ep.Length:765\n",
      "155 Rewards:-21.0, Loss:-54.88507, Ep.Length:765\n",
      "156 Rewards:-21.0, Loss:-55.13223, Ep.Length:765\n",
      "157 Rewards:-21.0, Loss:-55.32057, Ep.Length:765\n",
      "158 Rewards:-21.0, Loss:-55.52618, Ep.Length:765\n",
      "159 Rewards:-21.0, Loss:-55.69210, Ep.Length:765\n",
      "160 Rewards:-20.0, Loss:-50.81602, Ep.Length:845\n",
      "161 Rewards:-21.0, Loss:-56.13708, Ep.Length:765\n",
      "162 Rewards:-21.0, Loss:-56.35007, Ep.Length:765\n",
      "163 Rewards:-21.0, Loss:-50.32080, Ep.Length:914\n",
      "164 Rewards:-20.0, Loss:-51.47862, Ep.Length:844\n",
      "165 Rewards:-21.0, Loss:-53.47392, Ep.Length:874\n",
      "166 Rewards:-20.0, Loss:-45.72464, Ep.Length:1052\n",
      "167 Rewards:-21.0, Loss:-57.27179, Ep.Length:765\n",
      "168 Rewards:-18.0, Loss:-43.32231, Ep.Length:1133\n",
      "169 Rewards:-21.0, Loss:-57.69156, Ep.Length:765\n",
      "170 Rewards:-21.0, Loss:-57.86110, Ep.Length:765\n",
      "171 Rewards:-20.0, Loss:-47.98865, Ep.Length:1000\n",
      "172 Rewards:-21.0, Loss:-58.21527, Ep.Length:765\n",
      "173 Rewards:-20.0, Loss:-52.80641, Ep.Length:872\n",
      "174 Rewards:-21.0, Loss:-58.64601, Ep.Length:765\n",
      "175 Rewards:-20.0, Loss:-52.24009, Ep.Length:871\n",
      "176 Rewards:-21.0, Loss:-59.02942, Ep.Length:765\n",
      "177 Rewards:-21.0, Loss:-56.29539, Ep.Length:850\n",
      "178 Rewards:-21.0, Loss:-59.38371, Ep.Length:765\n",
      "179 Rewards:-21.0, Loss:-52.12579, Ep.Length:944\n",
      "180 Rewards:-21.0, Loss:-55.76264, Ep.Length:885\n",
      "181 Rewards:-19.0, Loss:-47.39820, Ep.Length:1048\n",
      "182 Rewards:-21.0, Loss:-60.13371, Ep.Length:765\n",
      "183 Rewards:-21.0, Loss:-60.38439, Ep.Length:765\n",
      "184 Rewards:-21.0, Loss:-54.56874, Ep.Length:913\n",
      "185 Rewards:-21.0, Loss:-59.23515, Ep.Length:793\n",
      "186 Rewards:-21.0, Loss:-60.91385, Ep.Length:765\n",
      "187 Rewards:-21.0, Loss:-61.06792, Ep.Length:765\n",
      "188 Rewards:-21.0, Loss:-61.28508, Ep.Length:765\n",
      "189 Rewards:-20.0, Loss:-50.84946, Ep.Length:999\n",
      "190 Rewards:-21.0, Loss:-57.35923, Ep.Length:853\n",
      "191 Rewards:-21.0, Loss:-61.83517, Ep.Length:765\n",
      "192 Rewards:-21.0, Loss:-54.69395, Ep.Length:935\n",
      "193 Rewards:-21.0, Loss:-62.22750, Ep.Length:765\n",
      "194 Rewards:-21.0, Loss:-62.39183, Ep.Length:765\n",
      "195 Rewards:-21.0, Loss:-62.56301, Ep.Length:765\n",
      "196 Rewards:-21.0, Loss:-61.79334, Ep.Length:784\n",
      "197 Rewards:-21.0, Loss:-62.95566, Ep.Length:765\n",
      "198 Rewards:-21.0, Loss:-63.12727, Ep.Length:765\n",
      "199 Rewards:-21.0, Loss:-61.87825, Ep.Length:793\n",
      "200 Rewards:-18.0, Loss:-47.87754, Ep.Length:1094\n",
      "201 Rewards:-21.0, Loss:-63.70185, Ep.Length:765\n",
      "202 Rewards:-21.0, Loss:-58.64830, Ep.Length:911\n",
      "203 Rewards:-21.0, Loss:-63.49473, Ep.Length:793\n",
      "204 Rewards:-21.0, Loss:-64.23521, Ep.Length:765\n",
      "205 Rewards:-21.0, Loss:-58.79827, Ep.Length:883\n",
      "206 Rewards:-20.0, Loss:-55.60537, Ep.Length:931\n",
      "207 Rewards:-21.0, Loss:-64.78989, Ep.Length:765\n",
      "208 Rewards:-21.0, Loss:-63.95395, Ep.Length:784\n",
      "209 Rewards:-21.0, Loss:-65.15781, Ep.Length:765\n",
      "210 Rewards:-21.0, Loss:-65.31745, Ep.Length:765\n",
      "211 Rewards:-21.0, Loss:-65.51594, Ep.Length:765\n",
      "212 Rewards:-20.0, Loss:-55.71154, Ep.Length:965\n",
      "213 Rewards:-21.0, Loss:-62.93021, Ep.Length:825\n",
      "214 Rewards:-21.0, Loss:-66.07269, Ep.Length:765\n",
      "215 Rewards:-21.0, Loss:-66.24035, Ep.Length:765\n",
      "216 Rewards:-21.0, Loss:-62.86663, Ep.Length:875\n",
      "217 Rewards:-20.0, Loss:-55.97537, Ep.Length:978\n",
      "218 Rewards:-21.0, Loss:-65.70168, Ep.Length:814\n",
      "219 Rewards:-21.0, Loss:-66.97721, Ep.Length:765\n",
      "220 Rewards:-21.0, Loss:-61.22795, Ep.Length:887\n",
      "221 Rewards:-21.0, Loss:-67.31551, Ep.Length:765\n",
      "222 Rewards:-21.0, Loss:-67.52354, Ep.Length:765\n",
      "223 Rewards:-21.0, Loss:-63.10717, Ep.Length:855\n",
      "224 Rewards:-21.0, Loss:-67.87807, Ep.Length:765\n",
      "225 Rewards:-20.0, Loss:-64.29456, Ep.Length:843\n",
      "226 Rewards:-20.0, Loss:-64.43285, Ep.Length:844\n",
      "227 Rewards:-21.0, Loss:-59.29889, Ep.Length:973\n",
      "228 Rewards:-20.0, Loss:-63.02814, Ep.Length:839\n",
      "229 Rewards:-21.0, Loss:-68.78534, Ep.Length:765\n",
      "230 Rewards:-20.0, Loss:-62.55893, Ep.Length:905\n",
      "231 Rewards:-21.0, Loss:-69.15891, Ep.Length:765\n",
      "232 Rewards:-20.0, Loss:-56.34457, Ep.Length:1097\n",
      "233 Rewards:-21.0, Loss:-65.73755, Ep.Length:844\n",
      "234 Rewards:-20.0, Loss:-64.14602, Ep.Length:843\n",
      "235 Rewards:-21.0, Loss:-69.82483, Ep.Length:765\n",
      "236 Rewards:-21.0, Loss:-70.04562, Ep.Length:765\n",
      "237 Rewards:-21.0, Loss:-67.85123, Ep.Length:825\n",
      "238 Rewards:-21.0, Loss:-65.39605, Ep.Length:899\n",
      "239 Rewards:-21.0, Loss:-70.51402, Ep.Length:765\n",
      "240 Rewards:-21.0, Loss:-70.75851, Ep.Length:765\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m----> 3\u001B[0m     rewards, loss, length \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Rewards:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrewards\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.5f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Ep.Length:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlength\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m values \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m----> 8\u001B[0m     action, logp, value \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     obs, reward, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     10\u001B[0m     rewards\u001B[38;5;241m.\u001B[39mappend(reward)\n",
      "File \u001B[0;32m~/.pyenv/versions/rl_practice/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1182\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1184\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1185\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mActorCritic.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     29\u001B[0m         \u001B[38;5;66;03m# TODO add conv layers\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m#         set_trace()\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m         body \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m         body \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcnn(body)\n\u001B[1;32m     33\u001B[0m         body \u001B[38;5;241m=\u001B[39m body\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36mActorCritic.preprocess\u001B[0;34m(self, obs)\u001B[0m\n\u001B[1;32m     55\u001B[0m result[result \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     56\u001B[0m result[result \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 58\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_obs \u001B[38;5;241m=\u001B[39m obs\n\u001B[1;32m     60\u001B[0m result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mas_tensor(result,\n\u001B[1;32m     61\u001B[0m                          dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32,\n\u001B[1;32m     62\u001B[0m                          device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     63\u001B[0m result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39munsqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/.pyenv/versions/rl_practice/lib/python3.10/site-packages/torch/nn/modules/module.py:1264\u001B[0m, in \u001B[0;36mModule.__setattr__\u001B[0;34m(self, name, value)\u001B[0m\n\u001B[1;32m   1260\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m   1261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1262\u001B[0m         \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n\u001B[0;32m-> 1264\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__setattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name: \u001B[38;5;28mstr\u001B[39m, value: Union[Tensor, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModule\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1265\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mremove_from\u001B[39m(\u001B[38;5;241m*\u001B[39mdicts_or_sets):\n\u001B[1;32m   1266\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m dicts_or_sets:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%time\n",
    "for epoch in range(epochs):\n",
    "    rewards, loss, length = train_one_epoch()\n",
    "    print(f'{epoch} Rewards:{rewards}, Loss:{loss:.5f}, Ep.Length:{length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360b585",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f226f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name, render_mode='human')\n",
    "obs = env.reset()\n",
    "done = False\n",
    "with torch.no_grad():\n",
    "    while not done:\n",
    "        action, _, _ = agent(obs)\n",
    "        obs, _, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99302bb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "rl_practice",
   "language": "python",
   "name": "rl_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}